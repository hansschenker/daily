NLP

Token
----------------------------
· Tokens are pieces of words. Before the API processes
the prompt, the input is broken down into tokens.
. Tokens can be words or just chunks of characters.
· 1 token is approximately 4 characters or 0.75 words for
English text.
OpenAi API




Embedding
--------------------------


Vector
--------------------------


LLM
---------------------------
LLMs issues and limitations:
· capability
· alignment
Capability is the model's ability to perform a specific task or how
well it is able to optimize its objective function.
Alignment is concerned with what we actually want the model to
do versus what it has been trained to do (if model's goals and
behavior align with human values and expectations).

AT
Alignment problems in Large Language Models (LLMs):
· Model Hallucinations.
· Lack of interpretability.
· Generating biased or toxic output.

GPT-3 family of models are generative models.
Core techniques:
. Next-token Prediction
· Masked-language Modeling.
Next-token prediction: the model is given a sequence of words as
input and it is asked to predict in a reasonable human-like way the
next word in the sequence.


Langchain
--------------------------

Pytorch

Jupyter

Jupyter Notebook

intro on yt: https://www.youtube.com/playlist?list=PLyRiRUsTyUXhatXwqLKLJuCeq_arZYRjG

Anaconda

Pandas

PandaAI

Train of Thought

Chain of Thought

Prompt

Prompt Engineering

LLM Fine-Tuning
-------------------------
openai.com/blog/gpt-3-5-turbo-fine-tuning-and-api-updates
platform.openai.com/docs/guides/fine-tuning



Python NumPy, Nvidia GPU

Langchain

Pinecone

Vector databases

Streamlit

Jupyter Lab

Jupyter Codespace

Google Colab (google account, goto new, create colab)
	
torch

keras

Openai

pip install openai
!pip install -q openai // -q for quiet installation

Openai Codex



ChatGPT

The models that power ChatGPT focus on the following
areas:
· Large Language Models (LLM)
· Self-Attention Mechanism
. Reinforcement Learning from Human Feedback (RLHF).


GPT - "Generative Pre-trained Transformer".
A transformer is a deep learning model that adopts the
mechanism of self-attention, to differentiate the
significance of each part of the input data using weights.


